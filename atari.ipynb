{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-07T19:17:12.492929Z",
     "start_time": "2024-12-07T19:17:10.998139Z"
    }
   },
   "source": [
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:17:14.975214Z",
     "start_time": "2024-12-07T19:17:14.968860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, nb_actions):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(nn.Conv2d(4, 16, 8, stride=4), nn.ReLU(),\n",
    "                                     nn.Conv2d(16, 32, 4, stride=2), nn.ReLU(),\n",
    "                                     nn.Flatten(), nn.Linear(2592, 256), nn.ReLU(),\n",
    "                                     nn.Linear(256, nb_actions), )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x / 255.)"
   ],
   "id": "fed925d66745522b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "15762d6378c60bcc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Initialize replay memory D to capacity N\n",
    "Initialize action-value function Q with random weights\n",
    "\n",
    "for episode = 1, M do\n",
    "    Initialise sequence s₁ = {x₁} and preprocessed sequenced φ₁ = φ(s₁)\n",
    "\n",
    "    for t = 1, T do\n",
    "        With probability ε select a random action aₜ\n",
    "        otherwise select aₜ = max_a Q*(φ(sₜ), a; θ)\n",
    "\n",
    "        Execute action aₜ in emulator and observe reward rₜ and image xₜ₊₁\n",
    "\n",
    "        Set sₜ₊₁ = sₜ, aₜ, xₜ₊₁ and preprocess φₜ₊₁ = φ(sₜ₊₁)\n",
    "\n",
    "        Store transition (φₜ, aₜ, rₜ, φₜ₊₁) in D\n",
    "\n",
    "        Sample random minibatch of transitions (φⱼ, aⱼ, rⱼ, φⱼ₊₁) from D\n",
    "\n",
    "        Set yⱼ = {\n",
    "            rⱼ                                    for terminal φⱼ₊₁\n",
    "            rⱼ + γ max_a' Q(φⱼ₊₁, a'; θ)         for non-terminal φⱼ₊₁\n",
    "        }\n",
    "\n",
    "        Perform a gradient descent step on (yⱼ − Q(φⱼ, aⱼ; θ))² according to equation 3\n",
    "    end for\n",
    "end for"
   ],
   "id": "57d14571f169e1b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a91b31a8e9ba2c43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:41:21.253384Z",
     "start_time": "2024-12-07T19:41:21.230799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def Deep_Q_Learning(env, replay_memory_size=100_000, nb_epochs=30_000_000, update_frequency=4, batch_size=32,\n",
    "                    discount_factor=0.99, replay_start_size=80_000, initial_exploration=1, final_exploration=0.01,\n",
    "                    exploration_steps=1_000_000, device='cuda'):\n",
    "    \n",
    "    # Initialize replay memory D to capacity N\n",
    "    rb = ReplayBuffer(replay_memory_size, env.observation_space, env.action_space, device,\n",
    "                      optimize_memory_usage=True, handle_timeout_termination=False)\n",
    "    \n",
    "    # Initialize action-value function Q with random weights\n",
    "    q_network = DQN(env.action_space.n).to(device)\n",
    "    optimizer = torch.optim.Adam(q_network.parameters(), lr=1.25e-4)\n",
    "\n",
    "    epoch = 0\n",
    "    smoothed_rewards = []\n",
    "    rewards = []\n",
    "\n",
    "    # Update progress bar to only refresh every 1000 steps\n",
    "    progress_bar = tqdm(total=nb_epochs, miniters=1000, unit_scale=True)\n",
    "\n",
    "    while epoch <= nb_epochs:\n",
    "        # Initialise sequence s1 = {x1}and preprocessed sequenced φ1 = φ(s1)\n",
    "        dead = False\n",
    "        total_rewards = 0\n",
    "        obs = env.reset()[0]\n",
    "\n",
    "        for _ in range(random.randint(1, 30)):\n",
    "            obs, _, terminated, truncated, info = env.step(1)\n",
    "\n",
    "        # for t= 1,T do\n",
    "        while not dead:\n",
    "            current_life = info['lives']\n",
    "\n",
    "            epsilon = max((final_exploration - initial_exploration) / exploration_steps * epoch + initial_exploration,\n",
    "                          final_exploration)\n",
    "            # With probability ϵ select a random action a\n",
    "            if random.random() < epsilon:\n",
    "                action = np.array(env.action_space.sample())\n",
    "                # otherwise select at = maxa Q∗(φ(st),a; θ)\n",
    "            else:\n",
    "                q_values = q_network(torch.Tensor(obs).unsqueeze(0).to(device))\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "            '''\n",
    "            Breaking down each parameter:\n",
    "            The ALE emulator creates a virtual frame buffer that represents the Atari 2600's video output. \n",
    "            \n",
    "            The raw output from ALE is:\n",
    "\n",
    "            A frame buffer of 160x210 pixels in RGB format\n",
    "            The current RAM state of the emulated Atari\n",
    "            The current score/reward from the game memory\n",
    "            The current game state (lives, game over, etc.)\n",
    "\n",
    "            When used through Gym/Gymnasium, this gets processed:\n",
    "\n",
    "            The frame buffer gets preprocessed:\n",
    "\n",
    "            Converted to grayscale (usually)\n",
    "            Often downscaled to 84x84 pixels\n",
    "            Frames are often stacked (4 frames is common) to give temporal information\n",
    "            Back from the ale via the gym wrapper\n",
    "            next_obs: The screen data (observation) after taking the action\n",
    "            reward: The score change from the action\n",
    "            terminated: True if the game naturally ended (like losing all lives)\n",
    "            truncated: True if the episode was artificially cut off (like reaching max steps)\n",
    "            info: Dictionary with additional information like current lives\n",
    "            '''\n",
    "\n",
    "            # Execute action at in emulator and observe reward rt and image xt+1\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            dead = terminated or truncated\n",
    "\n",
    "            done = True if (info['lives'] < current_life) else False\n",
    "            # Set st+1 = st,at,xt+1 and preprocess φt+1 = φ(st+1)\n",
    "            real_next_obs = next_obs.copy()\n",
    "            total_rewards += reward\n",
    "            reward = np.sign(reward)\n",
    "\n",
    "            # Store transition (φt,at,rt,φt+1) in D\n",
    "            rb.add(obs, real_next_obs, action, reward, done, info)\n",
    "            obs = next_obs\n",
    "\n",
    "            if epoch > replay_start_size and epoch % update_frequency == 0:\n",
    "                # Sample random minibatch of transitions (φj,aj,rj,φj+1) from D\n",
    "                data = rb.sample(batch_size) # Get batch of past experiences\n",
    "\n",
    "                # Set yj to rj + γ maxa′ Q(φj+1,a′; θ)\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    # What ACTUALLY happened , # Prediction for FUTURE value (next state)\n",
    "                    max_q_value, _ = q_network(data.next_observations).max(dim=1)\n",
    "                    # Target combines REAL reward with PREDICTED future value\n",
    "                    # Combine the REAL reward we got with our prediction of future value\n",
    "                    # This becomes our target - what we think the total value should have been\n",
    "                    # This is implementing Q(s,a) = r + γ * max[Q(s',a')]\n",
    "                    y = data.rewards.flatten() + discount_factor * max_q_value * (1 - data.dones.flatten())\n",
    "\n",
    "                # What it PREDICTED, # Prediction for the state when we took the action\n",
    "                # Then later, current_q_value is asking:\n",
    "                # \"What did you think that action would be worth before you took it?\"\n",
    "                current_q_value = q_network(data.observations).gather(1, data.actions).squeeze()\n",
    "                \n",
    "                # Learn from the difference\n",
    "                # Perform a gradient descent step on (yj−Q(φj,aj; θ))2 according to equation 3\n",
    "                loss = F.huber_loss(y, current_q_value)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            epoch += 1\n",
    "            # Only update progress bar every 1000 steps\n",
    "            if epoch % 1000 == 0:\n",
    "                progress_bar.update(1000)\n",
    "\n",
    "            if (epoch % 50_000 == 0) and epoch > 0:\n",
    "                smoothed_rewards.append(np.mean(rewards))\n",
    "                rewards = []\n",
    "                plt.plot(smoothed_rewards)\n",
    "                plt.title(\"Average Reward on Breakout\")\n",
    "                plt.xlabel(\"Training Epochs\")\n",
    "                plt.ylabel(\"Average Reward per Episode\")\n",
    "                #plt.savefig('Imgs/average_reward_on_breakout.png')\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "        rewards.append(total_rewards)"
   ],
   "id": "c7a8e2c008708f17",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c98660c93365245c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:36:08.149995Z",
     "start_time": "2024-12-07T19:36:08.146992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!pip install --upgrade pip setuptools wheel\n",
    "#!pip install opencv-python\n",
    "#!pip install \"gym[atari]\""
   ],
   "id": "550000b92f21a25c",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:36:08.750420Z",
     "start_time": "2024-12-07T19:36:08.748334Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "128c8afeb989bd69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:36:09.262886Z",
     "start_time": "2024-12-07T19:36:09.261142Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f5c587f4ba134cd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:36:09.781580Z",
     "start_time": "2024-12-07T19:36:09.779155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First remove existing installations to avoid conflicts\n",
    "#!pip uninstall gym ale-py AutoROM atari-py -y\n",
    "\n",
    "# Then install in the correct order\n",
    "#!pip install gym\n",
    "#!pip install \"gym[atari]\"\n",
    "#!pip install ale-py\n",
    "#!pip install \"autorom[accept-rom-license]\"\n",
    "#!python -m autorom\n",
    "\n",
    "# Or you can do it all in one line:\n",
    "#!pip install gym \"gym[atari]\" ale-py \"autorom[accept-rom-license]\" && python -m autorom"
   ],
   "id": "5f3dd4ef67b608d9",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:36:10.449543Z",
     "start_time": "2024-12-07T19:36:10.447256Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3b9053a174649b8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:36:10.949969Z",
     "start_time": "2024-12-07T19:36:10.947165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "all_envs = gym.envs.registry.keys()\n",
    "for env in sorted(all_envs):\n",
    "    print(env)"
   ],
   "id": "afa0fd5c7b585245",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acrobot-v1\n",
      "Ant-v2\n",
      "Ant-v3\n",
      "Ant-v4\n",
      "Ant-v5\n",
      "BipedalWalker-v3\n",
      "BipedalWalkerHardcore-v3\n",
      "Blackjack-v1\n",
      "CarRacing-v3\n",
      "CartPole-v0\n",
      "CartPole-v1\n",
      "CliffWalking-v0\n",
      "FrozenLake-v1\n",
      "FrozenLake8x8-v1\n",
      "GymV21Environment-v0\n",
      "GymV26Environment-v0\n",
      "HalfCheetah-v2\n",
      "HalfCheetah-v3\n",
      "HalfCheetah-v4\n",
      "HalfCheetah-v5\n",
      "Hopper-v2\n",
      "Hopper-v3\n",
      "Hopper-v4\n",
      "Hopper-v5\n",
      "Humanoid-v2\n",
      "Humanoid-v3\n",
      "Humanoid-v4\n",
      "Humanoid-v5\n",
      "HumanoidStandup-v2\n",
      "HumanoidStandup-v4\n",
      "HumanoidStandup-v5\n",
      "InvertedDoublePendulum-v2\n",
      "InvertedDoublePendulum-v4\n",
      "InvertedDoublePendulum-v5\n",
      "InvertedPendulum-v2\n",
      "InvertedPendulum-v4\n",
      "InvertedPendulum-v5\n",
      "LunarLander-v3\n",
      "LunarLanderContinuous-v3\n",
      "MountainCar-v0\n",
      "MountainCarContinuous-v0\n",
      "Pendulum-v1\n",
      "Pusher-v2\n",
      "Pusher-v4\n",
      "Pusher-v5\n",
      "Reacher-v2\n",
      "Reacher-v4\n",
      "Reacher-v5\n",
      "Swimmer-v2\n",
      "Swimmer-v3\n",
      "Swimmer-v4\n",
      "Swimmer-v5\n",
      "Taxi-v3\n",
      "Walker2d-v2\n",
      "Walker2d-v3\n",
      "Walker2d-v4\n",
      "Walker2d-v5\n",
      "phys2d/CartPole-v0\n",
      "phys2d/CartPole-v1\n",
      "phys2d/Pendulum-v0\n",
      "tabular/Blackjack-v0\n",
      "tabular/CliffWalking-v0\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:36:11.762551Z",
     "start_time": "2024-12-07T19:36:11.759872Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "fa058646218f1c40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:36:12.353791Z",
     "start_time": "2024-12-07T19:36:12.349767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "all_envs = gym.envs.registry.keys()\n",
    "atari_envs = [env for env in all_envs if 'Breakout' in env]\n",
    "print(\"\\nAtari Breakout environments:\")\n",
    "for env in sorted(atari_envs):\n",
    "    print(env)"
   ],
   "id": "18405f0ebb96751e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Atari Breakout environments:\n",
      "ALE/Breakout-ram-v5\n",
      "ALE/Breakout-v5\n",
      "Breakout-ram-v0\n",
      "Breakout-ram-v4\n",
      "Breakout-ramDeterministic-v0\n",
      "Breakout-ramDeterministic-v4\n",
      "Breakout-ramNoFrameskip-v0\n",
      "Breakout-ramNoFrameskip-v4\n",
      "Breakout-v0\n",
      "Breakout-v4\n",
      "BreakoutDeterministic-v0\n",
      "BreakoutDeterministic-v4\n",
      "BreakoutNoFrameskip-v0\n",
      "BreakoutNoFrameskip-v4\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:36:13.359153Z",
     "start_time": "2024-12-07T19:36:13.268762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
    "\n",
    "print(\"Observation Space: \", env.observation_space)\n",
    "print(\"Action Space       \", env.action_space)\n"
   ],
   "id": "86c7f3898107900c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space:  Box(0, 255, (210, 160, 3), uint8)\n",
      "Action Space        Discrete(4)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:36:14.158065Z",
     "start_time": "2024-12-07T19:36:14.155213Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9b004b4e224731fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:41:49.675936Z",
     "start_time": "2024-12-07T19:41:49.666781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        terminated = truncated = False\n",
    "\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, term, trunc, info = self.env.step(action)\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            terminated = terminated or term\n",
    "            truncated = truncated or trunc\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "        return max_frame, total_reward, terminated, truncated, info"
   ],
   "id": "4d931942b5221b30",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:36:15.502401Z",
     "start_time": "2024-12-07T19:36:15.500167Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9c241cddb196181e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv\n",
    "\n",
    "\n",
    "env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "env = gym.wrappers.GrayScaleObservation(env)\n",
    "env = gym.wrappers.FrameStack(env, 4) # 3 frames for space invaders\n",
    "env = MaxAndSkipEnv(env, skip=4)\n",
    "\n",
    "Deep_Q_Learning(env, replay_memory_size=100_000, device='cpu')\n",
    "env.close()"
   ],
   "id": "b19d564aaaba288d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "33d451da8b410b40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T15:31:25.563302Z",
     "start_time": "2024-12-11T15:31:25.458868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the currently running model\n",
    "torch.save({\n",
    "    'model_state_dict': q_network.state_dict(),\n",
    "    'action_space': env.action_space.n,\n",
    "}, 'dqn_checkpoint_manual.pt')"
   ],
   "id": "c4280536371277a0",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q_network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Save the currently running model\u001B[39;00m\n\u001B[1;32m      2\u001B[0m torch\u001B[38;5;241m.\u001B[39msave({\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_state_dict\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[43mq_network\u001B[49m\u001B[38;5;241m.\u001B[39mstate_dict(),\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124maction_space\u001B[39m\u001B[38;5;124m'\u001B[39m: env\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mn,\n\u001B[1;32m      5\u001B[0m }, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdqn_checkpoint_manual.pt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'q_network' is not defined"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T19:17:47.602836Z",
     "start_time": "2024-12-07T19:17:47.598702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium\n",
    "print(gymnasium.__version__)"
   ],
   "id": "5a39a309df7dbf9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a7b461332bd3d362"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9a3f75c71e9894e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from stable_baselines3.common.off_policy_algorithm import OffPolicyAlgorithm\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.utils import safe_mean\n",
    "\n",
    "import torch as th\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class CustomDQN(OffPolicyAlgorithm):\n",
    "    def __init__(\n",
    "            self,\n",
    "            policy,\n",
    "            env,\n",
    "            replay_memory_size=100_000,\n",
    "            nb_epochs=30_000_000,\n",
    "            update_frequency=4,\n",
    "            batch_size=32,\n",
    "            discount_factor=0.99,\n",
    "            replay_start_size=80_000,\n",
    "            initial_exploration=1.0,\n",
    "            final_exploration=0.01,\n",
    "            exploration_steps=1_000_000,\n",
    "            learning_rate=1.25e-4,\n",
    "            device=\"cuda\",\n",
    "            **kwargs\n",
    "    ):\n",
    "        super(CustomDQN, self).__init__(policy, env, device=device, **kwargs)\n",
    "        self.replay_buffer = ReplayBuffer(\n",
    "            replay_memory_size, env.observation_space, env.action_space, device, optimize_memory_usage=True\n",
    "        )\n",
    "        self.q_network = self.policy.q_net  # Ensure your policy contains a q_net\n",
    "        self.optimizer = th.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.epochs = nb_epochs\n",
    "        self.update_frequency = update_frequency\n",
    "        self.batch_size = batch_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.replay_start_size = replay_start_size\n",
    "        self.initial_exploration = initial_exploration\n",
    "        self.final_exploration = final_exploration\n",
    "        self.exploration_steps = exploration_steps\n",
    "        self.smoothed_rewards = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def train(self, gradient_steps: int, batch_size: int = 32):\n",
    "        for epoch in range(self.epochs):\n",
    "            obs = self.env.reset()[0]\n",
    "            dead = False\n",
    "            total_rewards = 0\n",
    "\n",
    "            while not dead:\n",
    "                epsilon = max(\n",
    "                    (self.final_exploration - self.initial_exploration) / self.exploration_steps * epoch\n",
    "                    + self.initial_exploration,\n",
    "                    self.final_exploration,\n",
    "                )\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    q_values = self.q_network(th.Tensor(obs).unsqueeze(0).to(self.device))\n",
    "                    action = th.argmax(q_values, dim=1).item()\n",
    "\n",
    "                next_obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "                dead = terminated or truncated\n",
    "\n",
    "                self.replay_buffer.add(obs, next_obs, action, reward, terminated)\n",
    "                obs = next_obs\n",
    "                total_rewards += reward\n",
    "\n",
    "                if epoch > self.replay_start_size and epoch % self.update_frequency == 0:\n",
    "                    # Sample from the replay buffer\n",
    "                    data = self.replay_buffer.sample(self.batch_size)\n",
    "                    with th.no_grad():\n",
    "                        max_q_values = self.q_network(data.next_observations).max(1)[0]\n",
    "                        targets = data.rewards.flatten() + self.discount_factor * max_q_values * (1 - data.dones.flatten())\n",
    "\n",
    "                    # Compute loss and optimize\n",
    "                    q_values = self.q_network(data.observations).gather(1, data.actions).squeeze()\n",
    "                    loss = F.huber_loss(q_values, targets)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "            self.rewards.append(total_rewards)\n",
    "            if epoch % 1000 == 0:\n",
    "                self.logger.record(\"train/mean_reward\", safe_mean(self.rewards[-100:]))\n"
   ],
   "id": "63e6861468912f39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e17158282f26f6c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Modify the RL Zoo registry (train.py):\n",
    "\n"
   ],
   "id": "788589ccc2ea5125"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from custom_dqn import CustomDQN\n",
    "\n",
    "ALGOS[\"custom_dqn\"] = CustomDQN\n"
   ],
   "id": "77b327c9d01941f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from stable_baselines3.common.policies import BasePolicy\n",
    "from torch import nn\n",
    "\n",
    "class CustomDQNPolicy(BasePolicy):\n",
    "    def __init__(self, observation_space, action_space, lr_schedule):\n",
    "        super(CustomDQNPolicy, self).__init__(observation_space, action_space, lr_schedule)\n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(observation_space.shape[0], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_space.n)\n",
    "        )\n",
    "        self.optimizer = th.optim.Adam(self.parameters(), lr=lr_schedule(1))\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.q_net(obs)\n"
   ],
   "id": "8884dede5528d127"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "112e4cd75ff9e847"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2a4ae4d6ba51a1d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "87af0a8576e5e6de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "76e5bbb848081eaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Train the Algorithm\n",
    "Now you can use the Zoo CLI to train your CustomDQN:"
   ],
   "id": "fbb00bfdccd5bee3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "python train.py --algo custom_dqn --env CartPole-v1 --hyperparams path/to/params.json\n",
   "id": "73aa71f96169b972"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### related methods\n",
   "id": "e5b29a803f44bdb1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Deep Neural Networks for Environment Estimation:\n",
    "They learn patterns from raw input data through multiple layers of processing to predict how the environment will respond to actions.\n",
    "## Restricted Boltzmann Machines:\n",
    "They create a two-way (bidirectional) network that can learn complex patterns by repeatedly comparing and adjusting between visible input data and hidden learned features.\n",
    "##Gradient Temporal-Difference Methods:\n",
    "They improve learning stability by updating the neural network weights based on the difference between predicted and actual outcomes, but in a way that prevents the predictions from spiraling out of control.\n",
    "## Neural Fitted Q-learning:\n",
    "It updates the entire Q-value prediction network all at once using stored experiences to minimize prediction errors across all seen situations.\n",
    "## Experience Replay with Neural Networks:\n",
    " It stores past experiences in memory and randomly replays them during training to help the neural network learn more efficiently from less data.\n",
    "## HyperNEAT on Atari:\n",
    " It evolves neural networks through artificial evolution, using special rules that help it discover patterns across the game screen's spatial layout."
   ],
   "id": "852170d6ab63abfa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
