{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I-LoRA Implementation for Face Intrinsics\n",
    "Using FFHQ dataset and generating pseudo-ground truth using pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision diffusers transformers timm omegaconf zoedepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from zoedepth.models.builder import build_model\n",
    "from zoedepth.utils.config import get_config\n",
    "import requests\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Pseudo Ground Truth Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class IntrinsicEstimator:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Initialize depth estimator (ZoeDepth)\n",
    "        PRETRAINED_ZOEDEPTH = \"https://github.com/isl-org/ZoeDepth/releases/download/v1.0/ZoeD_M12_N.pt\"\n",
    "        conf = OmegaConf.load(\"zoedepth/configs/zoedepth_nk.yaml\")\n",
    "        self.depth_model = build_model(conf)\n",
    "        self.depth_model.load_state_dict(torch.load(PRETRAINED_ZOEDEPTH))\n",
    "        self.depth_model.to(self.device).eval()\n",
    "        \n",
    "        # Download and setup Omnidata model for normals (simplified for example)\n",
    "        PRETRAINED_NORMALS = \"path_to_omnidata_normal_model\"\n",
    "        # Initialize normal estimator (you would need to implement this based on Omnidata)\n",
    "        \n",
    "        # Standard image transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def estimate_depth(self, image):\n",
    "        \"\"\"Estimate depth using ZoeDepth\"\"\"\n",
    "        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        depth = self.depth_model(image_tensor)\n",
    "        return depth.cpu().numpy()[0]\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def estimate_normals(self, image):\n",
    "        \"\"\"Estimate surface normals using Omnidata (placeholder)\"\"\"\n",
    "        # Implement normal estimation using Omnidata\n",
    "        pass\n",
    "    \n",
    "    def visualize_estimates(self, image):\n",
    "        \"\"\"Visualize original image and estimated intrinsics\"\"\"\n",
    "        depth = self.estimate_depth(image)\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(image)\n",
    "        plt.title('Original Image')\n",
    "        \n",
    "        plt.subplot(132)\n",
    "        plt.imshow(depth, cmap='magma')\n",
    "        plt.title('Estimated Depth')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Face Dataset with Pseudo Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class FaceIntrinsicDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, num_samples=250):\n",
    "        \"\"\"Dataset for faces with estimated intrinsics\n",
    "        \n",
    "        Args:\n",
    "            root_dir: Directory containing FFHQ images\n",
    "            transform: Optional transforms\n",
    "            num_samples: Number of samples to use (paper uses as few as 250)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.estimator = IntrinsicEstimator()\n",
    "        \n",
    "        # Get all image paths and limit to num_samples\n",
    "        self.image_paths = sorted(glob.glob(f\"{root_dir}/*.png\"))[:num_samples]\n",
    "        \n",
    "        # Pre-compute pseudo ground truth\n",
    "        print(\"Generating pseudo ground truth...\")\n",
    "        self.cached_intrinsics = {}\n",
    "        for idx, path in enumerate(self.image_paths):\n",
    "            image = Image.open(path)\n",
    "            self.cached_intrinsics[path] = {\n",
    "                'depth': self.estimator.estimate_depth(image),\n",
    "                # Add other intrinsics as needed\n",
    "            }\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"Processed {idx+1}/{len(self.image_paths)} images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get cached intrinsics\n",
    "        intrinsics = self.cached_intrinsics[image_path]\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'depth': torch.from_numpy(intrinsics['depth']),\n",
    "            # Add other intrinsics as needed\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example Usage with FFHQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Setup dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset with 250 samples as used in paper\n",
    "dataset = FaceIntrinsicDataset(\n",
    "    root_dir=\"path_to_ffhq_dataset\",  # Replace with your FFHQ path\n",
    "    transform=transform,\n",
    "    num_samples=250\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Visualize some examples\n",
    "def show_batch(batch):\n",
    "    images = batch['image']\n",
    "    depths = batch['depth']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    for i in range(4):\n",
    "        axes[0, i].imshow(images[i].permute(1, 2, 0))\n",
    "        axes[0, i].set_title('Original')\n",
    "        axes[1, i].imshow(depths[i], cmap='magma')\n",
    "        axes[1, i].set_title('Depth')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show a batch\n",
    "batch = next(iter(dataloader))\n",
    "show_batch(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 }
}
