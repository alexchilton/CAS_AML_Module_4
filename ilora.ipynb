{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-25T07:09:39.605796Z",
     "start_time": "2024-11-25T07:09:39.554498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set MPS memory limit before importing torch\n",
    "import os\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import DPTFeatureExtractor, DPTForDepthEstimation\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# LoRA layer implementation\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, rank=8):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.rank = rank\n",
    "\n",
    "        # Initialize LoRA matrices\n",
    "        self.lora_A = nn.Parameter(torch.zeros(in_dim, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, in_dim))\n",
    "        self.scale = 0.1\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_shape = x.shape\n",
    "        if len(x.shape) > 2:\n",
    "            x = x.view(-1, x.shape[-1])\n",
    "\n",
    "        if x.shape[-1] != self.in_dim:\n",
    "            raise ValueError(f\"Input dimension {x.shape[-1]} doesn't match LoRA dimension {self.in_dim}\")\n",
    "\n",
    "        lora_contribution = (x @ self.lora_A) @ self.lora_B\n",
    "        output = x + self.scale * lora_contribution\n",
    "        return output.view(original_shape)\n",
    "\n",
    "# Main I-LoRA implementation\n",
    "class ILoRA:\n",
    "    def __init__(self, model_id=\"CompVis/stable-diffusion-v1-4\", rank=8):\n",
    "        # Specific setup for M1 Mac\n",
    "        self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "    \n",
    "        # Clear cache before loading models\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "    \n",
    "        # Load models in stages with memory clearing between loads\n",
    "        print(\"Loading Stable Diffusion...\")\n",
    "        self.model = StableDiffusionPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float32,\n",
    "            safety_checker=None\n",
    "        ).to('cpu')  # Keep on CPU initially\n",
    "    \n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "    \n",
    "        print(\"Loading depth estimator...\")\n",
    "        self.depth_estimator = DPTForDepthEstimation.from_pretrained(\n",
    "            \"Intel/dpt-large\",\n",
    "            torch_dtype=torch.float32\n",
    "        ).to('cpu')  # Keep on CPU initially\n",
    "    \n",
    "        print(\"Loading feature extractor...\")\n",
    "        self.feature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-large\")\n",
    "    \n",
    "        self.rank = rank\n",
    "    \n",
    "        # Freeze parameters while still on CPU\n",
    "        self._freeze_parameters()\n",
    "    \n",
    "        # Add LoRA layers while on CPU\n",
    "        self._add_lora_layers()\n",
    "    \n",
    "        # Ensure all model parameters require gradients\n",
    "        for param in self.model.unet.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.model.vae.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.model.text_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "        print(\"Moving models to device...\")\n",
    "        # Move models to device in stages\n",
    "        try:\n",
    "            if torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "            self.depth_estimator.to(self.device)\n",
    "    \n",
    "            if torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "            self.model.to(self.device)\n",
    "    \n",
    "        except RuntimeError as e:\n",
    "            print(\"Memory error during model loading. Trying to free memory...\")\n",
    "            if torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "            raise e\n",
    "\n",
    "    def _freeze_parameters(self):\n",
    "        \"\"\"Freeze all parameters in the base model\"\"\"\n",
    "        # Freeze UNet parameters\n",
    "        for param in self.model.unet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Freeze VAE parameters\n",
    "        for param in self.model.vae.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Freeze text encoder parameters\n",
    "        for param in self.model.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def _add_lora_layers(self):\n",
    "        self.lora_layers = []\n",
    "\n",
    "        def create_attention_forward(original_forward, lora):\n",
    "            def new_forward(self, x, *args, **kwargs):\n",
    "                x = lora(x)\n",
    "                return original_forward(self, x, *args, **kwargs)\n",
    "            return new_forward\n",
    "    \n",
    "        def create_custom_forward(original_forward, lora, heads):\n",
    "            def new_forward(self, hidden_states, encoder_hidden_states=None, attention_mask=None, **kwargs):\n",
    "                batch_size, sequence_length, _ = hidden_states.shape\n",
    "    \n",
    "                query = self.to_q(hidden_states)\n",
    "                query = lora(query)\n",
    "    \n",
    "                key = self.to_k(encoder_hidden_states if encoder_hidden_states is not None else hidden_states)\n",
    "                value = self.to_v(encoder_hidden_states if encoder_hidden_states is not None else hidden_states)\n",
    "    \n",
    "                # Use head size from query projection dimensions\n",
    "                head_size = query.shape[-1] // heads\n",
    "    \n",
    "                # Reshape attention patterns\n",
    "                query = query.reshape(batch_size, -1, heads, head_size).transpose(1, 2)\n",
    "                key = key.reshape(batch_size, -1, heads, head_size).transpose(1, 2)\n",
    "                value = value.reshape(batch_size, -1, heads, head_size).transpose(1, 2)\n",
    "    \n",
    "                # Compute attention scores\n",
    "                attention_scores = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(head_size)\n",
    "    \n",
    "                if attention_mask is not None:\n",
    "                    attention_scores = attention_scores + attention_mask\n",
    "    \n",
    "                attention_probs = attention_scores.softmax(dim=-1)\n",
    "                hidden_states = torch.matmul(attention_probs, value)\n",
    "    \n",
    "                # Reshape back\n",
    "                hidden_states = hidden_states.transpose(1, 2).contiguous()\n",
    "                hidden_states = hidden_states.reshape(batch_size, -1, heads * head_size)\n",
    "    \n",
    "                # Output projection\n",
    "                hidden_states = self.to_out[0](hidden_states)\n",
    "                hidden_states = self.to_out[1](hidden_states)\n",
    "    \n",
    "                return hidden_states\n",
    "            return new_forward\n",
    "    \n",
    "        for name, module in self.model.unet.named_modules():\n",
    "            if isinstance(module, torch.nn.MultiheadAttention):\n",
    "                in_dim = module.embed_dim\n",
    "                lora = LoRALayer(in_dim, self.rank).to(self.device)\n",
    "                self.lora_layers.append(lora)\n",
    "    \n",
    "                module.forward = create_attention_forward(\n",
    "                    module.forward,\n",
    "                    lora\n",
    "                ).__get__(module)\n",
    "    \n",
    "            elif \"attn\" in name and hasattr(module, 'to_q'):\n",
    "                in_dim = module.to_q.out_features\n",
    "                lora = LoRALayer(in_dim, self.rank).to(self.device)\n",
    "    \n",
    "                module.forward = create_custom_forward(\n",
    "                    module.forward,\n",
    "                    lora,\n",
    "                    module.heads\n",
    "                ).__get__(module)\n",
    "    \n",
    "                self.lora_layers.append(lora)\n",
    "\n",
    "    def estimate_depth(self, images):\n",
    "        \"\"\"Estimate depth using DPT model\"\"\"\n",
    "        if torch.is_tensor(images):\n",
    "            images = [transforms.ToPILImage()(img.cpu()) for img in images]\n",
    "    \n",
    "        inputs = self.feature_extractor(images=images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = self.depth_estimator(**inputs)\n",
    "            predicted_depth = outputs.predicted_depth\n",
    "    \n",
    "            depth_min = predicted_depth.min()\n",
    "            depth_max = predicted_depth.max()\n",
    "            predicted_depth = (predicted_depth - depth_min) / (depth_max - depth_min)\n",
    "    \n",
    "        return predicted_depth\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        try:\n",
    "            # Clear MPS cache\n",
    "            if torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "\n",
    "            images = batch['image'].to(self.device)\n",
    "            print(f\"Input image shape: {images.shape}\")\n",
    "\n",
    "            depths = self.estimate_depth(images)\n",
    "\n",
    "            prompt = [\"depth map\"] * images.shape[0]\n",
    "\n",
    "            outputs = self.model(\n",
    "                prompt=prompt,\n",
    "                image=images,\n",
    "                num_inference_steps=10,  # Reduced for memory\n",
    "                guidance_scale=7.5\n",
    "            ).images\n",
    "\n",
    "            # Convert PIL images to tensors\n",
    "            outputs = torch.stack([transforms.ToTensor()(image) for image in outputs]).to(self.device)\n",
    "\n",
    "            # Convert outputs to grayscale by averaging the RGB channels\n",
    "            outputs = outputs.mean(dim=1, keepdim=True)\n",
    "\n",
    "            # Resize depths to match outputs dimensions\n",
    "            depths = depths.unsqueeze(1)  # Add channel dimension\n",
    "            depths = F.interpolate(depths, size=outputs.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Ensure depths tensor requires gradients\n",
    "            depths.requires_grad = True\n",
    "\n",
    "            loss = F.mse_loss(outputs, depths)\n",
    "\n",
    "            # Ensure loss requires gradients\n",
    "            assert loss.requires_grad, \"Loss does not require gradients\"\n",
    "\n",
    "            return loss, outputs\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                if torch.backends.mps.is_available():\n",
    "                    torch.mps.empty_cache()\n",
    "                raise e\n",
    "            raise e\n",
    "        \n",
    "    def predict(self, image):\n",
    "        with torch.no_grad():\n",
    "            if torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "\n",
    "            output = self.model(\n",
    "                prompt=[\"depth map\"],\n",
    "                image=image.unsqueeze(0),\n",
    "                num_inference_steps=10,\n",
    "                guidance_scale=7.5\n",
    "            ).images[0]\n",
    "            return output\n",
    "\n",
    "    def verify_dimensions(self):\n",
    "        \"\"\"Verify that all dimensions match correctly\"\"\"\n",
    "        print(\"\\nVerifying dimensions:\")\n",
    "        for name, module in self.model.unet.named_modules():\n",
    "            if hasattr(module, 'to_q'):\n",
    "                print(f\"\\nModule: {name}\")\n",
    "                print(f\"Query projection: {module.to_q.in_features} -> {module.to_q.out_features}\")\n",
    "                print(f\"Number of heads: {module.heads}\")\n",
    "                print(f\"Head size: {module.to_q.out_features // module.heads}\")\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T07:23:36.658966Z",
     "start_time": "2024-11-25T07:23:35.228107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset implementation\n",
    "\n",
    "class LocalFaceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Get all image files\n",
    "        self.image_files = []\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
    "            self.image_files.extend(glob.glob(os.path.join(root_dir, ext)))\n",
    "        print(f\"Found {len(self.image_files)} images in {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return {'image': image}\n",
    "\n",
    "# Set up dataset with memory-efficient parameters\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),  # Reduced size for memory efficiency\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "dataset = LocalFaceDataset(\n",
    "    root_dir=\"faces\",  # Your local directory with face images\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create subset of 250 samples as per paper\n",
    "indices = random.sample(range(len(dataset)), min(100, len(dataset)))\n",
    "dataset = Subset(dataset, indices)\n",
    "\n",
    "# Use small batch size for memory efficiency\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "print(f\"Created dataloader with {len(dataset)} samples\")\n",
    "\n",
    "# Training function\n",
    "def train_ilora(model, dataloader, num_epochs=10):\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for layer in model.lora_layers for p in layer.parameters() if p.requires_grad],\n",
    "        lr=1e-4\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            try:\n",
    "                # Training step\n",
    "                loss, predictions = model.train_step(batch)\n",
    "\n",
    "                # Ensure loss requires gradients\n",
    "                if not loss.requires_grad:\n",
    "                    raise RuntimeError(\"Loss does not require gradients\")\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "                # Clear memory\n",
    "                if torch.backends.mps.is_available():\n",
    "                    torch.mps.empty_cache()\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(\"Memory error during training. Clearing cache and skipping batch...\")\n",
    "                    if torch.backends.mps.is_available():\n",
    "                        torch.mps.empty_cache()\n",
    "                    continue\n",
    "                raise e\n",
    "\n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "                        # Save checkpoint\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'unet_state_dict': model.model.unet.state_dict(),\n",
    "                'vae_state_dict': model.model.vae.state_dict(),\n",
    "                'text_encoder_state_dict': model.model.text_encoder.state_dict(),\n",
    "                'depth_estimator_state_dict': model.depth_estimator.state_dict(),\n",
    "                'lora_layers_state_dict': [layer.state_dict() for layer in model.lora_layers],\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, f'ilora_checkpoint_epoch_{epoch+1}.pt')\n",
    "\n",
    "            # Visualize results for last batch\n",
    "            if len(dataloader) > 0:\n",
    "                sample_batch = next(iter(dataloader))\n",
    "                sample_image = sample_batch['image'][0]\n",
    "                pred_depth = model.predict(sample_image)\n",
    "\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.subplot(121)\n",
    "                plt.imshow(sample_image.permute(1, 2, 0))\n",
    "                plt.title('Original Image')\n",
    "                plt.subplot(122)\n",
    "                plt.imshow(pred_depth, cmap='magma')\n",
    "                plt.title(f'Predicted Depth (Epoch {epoch+1})')\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n"
   ],
   "id": "61b5dc61054f16f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202599 images in faces\n",
      "Created dataloader with 100 samples\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-25T07:23:38.635517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train\n",
    "try:\n",
    "    print(\"Initializing I-LoRA...\")\n",
    "    ilora = ILoRA()\n",
    "    ilora.verify_dimensions()\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_ilora(ilora, dataloader, num_epochs=10)\n",
    "\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e):\n",
    "        print(\"Memory error. Try reducing batch size or image size further.\")\n",
    "    raise e"
   ],
   "id": "51db236dfd14bc54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing I-LoRA...\n",
      "Using device: mps\n",
      "Loading Stable Diffusion...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "385f75cb22654638a8c6d90650ad8334"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading depth estimator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading feature extractor...\n",
      "Moving models to device...\n",
      "\n",
      "Verifying dimensions:\n",
      "\n",
      "Module: down_blocks.0.attentions.0.transformer_blocks.0.attn1\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: down_blocks.0.attentions.0.transformer_blocks.0.attn2\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: down_blocks.0.attentions.1.transformer_blocks.0.attn1\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: down_blocks.0.attentions.1.transformer_blocks.0.attn2\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: down_blocks.1.attentions.0.transformer_blocks.0.attn1\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: down_blocks.1.attentions.0.transformer_blocks.0.attn2\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: down_blocks.1.attentions.1.transformer_blocks.0.attn1\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: down_blocks.1.attentions.1.transformer_blocks.0.attn2\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: down_blocks.2.attentions.0.transformer_blocks.0.attn1\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: down_blocks.2.attentions.0.transformer_blocks.0.attn2\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: down_blocks.2.attentions.1.transformer_blocks.0.attn1\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: down_blocks.2.attentions.1.transformer_blocks.0.attn2\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: up_blocks.1.attentions.0.transformer_blocks.0.attn1\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: up_blocks.1.attentions.0.transformer_blocks.0.attn2\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: up_blocks.1.attentions.1.transformer_blocks.0.attn1\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: up_blocks.1.attentions.1.transformer_blocks.0.attn2\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: up_blocks.1.attentions.2.transformer_blocks.0.attn1\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: up_blocks.1.attentions.2.transformer_blocks.0.attn2\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: up_blocks.2.attentions.0.transformer_blocks.0.attn1\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: up_blocks.2.attentions.0.transformer_blocks.0.attn2\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: up_blocks.2.attentions.1.transformer_blocks.0.attn1\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: up_blocks.2.attentions.1.transformer_blocks.0.attn2\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: up_blocks.2.attentions.2.transformer_blocks.0.attn1\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: up_blocks.2.attentions.2.transformer_blocks.0.attn2\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: up_blocks.3.attentions.0.transformer_blocks.0.attn1\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: up_blocks.3.attentions.0.transformer_blocks.0.attn2\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: up_blocks.3.attentions.1.transformer_blocks.0.attn1\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: up_blocks.3.attentions.1.transformer_blocks.0.attn2\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: up_blocks.3.attentions.2.transformer_blocks.0.attn1\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: up_blocks.3.attentions.2.transformer_blocks.0.attn2\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: mid_block.attentions.0.transformer_blocks.0.attn1\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: mid_block.attentions.0.transformer_blocks.0.attn2\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Starting training...\n",
      "Input image shape: torch.Size([1, 3, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7a2fa2e66954a12913c8ab29040899b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 0, Loss: 0.1428\n",
      "Epoch 1/10, Average Loss: 0.0014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb798fad1cca48d09e32f1325585f05e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b077c2631196bc9c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
