{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T20:49:28.341749Z",
     "start_time": "2024-11-25T20:49:28.269982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "\n",
    "from transformers import AutoFeatureExtractor, AutoModelForDepthEstimation\n",
    "\n",
    "class FaceDepthEstimator:\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        self.model = AutoModelForDepthEstimation.from_pretrained(\"intel/dpt-hybrid-midas\")\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"intel/dpt-hybrid-midas\")\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def enhance_facial_features(self, depth):\n",
    "        # Increase contrast for facial features\n",
    "        mean_depth = depth.mean()\n",
    "        std_depth = depth.std()\n",
    "        depth = (depth - mean_depth) / (std_depth + 1e-6)\n",
    "\n",
    "        # Enhance prominent features (nose, eyes, mouth regions)\n",
    "        depth = torch.tanh(depth * 2) * 0.5 + 0.5\n",
    "        return depth\n",
    "\n",
    "    def bilateral_filter(self, depth):\n",
    "        # Adjusted parameters for face-specific filtering\n",
    "        sigma_space = 1.0  # Reduced for finer detail\n",
    "        sigma_color = 0.05  # More sensitive to depth changes\n",
    "\n",
    "        if len(depth.shape) == 2:\n",
    "            depth = depth.unsqueeze(0).unsqueeze(0)\n",
    "        elif len(depth.shape) == 3:\n",
    "            depth = depth.unsqueeze(0)\n",
    "\n",
    "        B, C, H, W = depth.shape\n",
    "\n",
    "        # Smaller kernel for detail preservation\n",
    "        kernel_size = 5\n",
    "        x = torch.linspace(-2, 2, kernel_size).to(self.device)\n",
    "        x = x.view(1, -1).repeat(kernel_size, 1)\n",
    "        y = x.t()\n",
    "        kernel = torch.exp(-(x**2 + y**2) / (2 * sigma_space**2))\n",
    "        kernel = kernel / kernel.sum()\n",
    "        kernel = kernel.to(self.device)\n",
    "\n",
    "        padded_depth = F.pad(depth, (kernel_size//2,)*4, mode='reflect')\n",
    "        output = torch.zeros_like(depth)\n",
    "\n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                patch = padded_depth[:, :, i:i+kernel_size, j:j+kernel_size]\n",
    "                center = depth[:, :, i:i+1, j:j+1]\n",
    "                color_weight = torch.exp(-(patch - center)**2 / (2 * sigma_color**2))\n",
    "                weight = kernel * color_weight\n",
    "                output[:, :, i:i+1, j:j+1] = (patch * weight).sum(dim=(2,3), keepdim=True) / weight.sum()\n",
    "\n",
    "        return output\n",
    "\n",
    "    def normalize_depth(self, depth):\n",
    "        # Face-specific normalization\n",
    "        depth = depth - depth.min()\n",
    "        depth = depth / (depth.max() + 1e-8)\n",
    "\n",
    "        # Emphasize mid-range depths where facial features are\n",
    "        depth = torch.pow(depth, 0.75)  # Adjust gamma for better feature visibility\n",
    "        return depth\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if torch.is_tensor(image):\n",
    "            image = transforms.ToPILImage()(image)\n",
    "\n",
    "        inputs = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            depth = self.model(**inputs).predicted_depth\n",
    "            depth = self.bilateral_filter(depth)\n",
    "            depth = self.enhance_facial_features(depth)\n",
    "            depth = self.normalize_depth(depth)\n",
    "\n",
    "        return depth"
   ],
   "id": "c2fdc7ebca2ca1e9",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-25T20:49:28.396471Z",
     "start_time": "2024-11-25T20:49:28.346559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set MPS memory limit before importing torch\n",
    "import os\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# LoRA layer implementation\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, rank=8):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.rank = rank\n",
    "\n",
    "        # Initialize LoRA matrices\n",
    "        self.lora_A = nn.Parameter(torch.zeros(in_dim, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, in_dim))\n",
    "        self.scale = 0.1\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_shape = x.shape\n",
    "        if len(x.shape) > 2:\n",
    "            x = x.view(-1, x.shape[-1])\n",
    "\n",
    "        if x.shape[-1] != self.in_dim:\n",
    "            raise ValueError(f\"Input dimension {x.shape[-1]} doesn't match LoRA dimension {self.in_dim}\")\n",
    "\n",
    "        lora_contribution = (x @ self.lora_A) @ self.lora_B\n",
    "        output = x + self.scale * lora_contribution\n",
    "        return output.view(original_shape)\n",
    "\n",
    "# Main I-LoRA implementation\n",
    "class ILoRA:\n",
    "    def __init__(self, model_id=\"CompVis/stable-diffusion-v1-4\", rank=8):\n",
    "        # Specific setup for M1 Mac\n",
    "        self.device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "    \n",
    "        # Clear cache before loading models\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "    \n",
    "        # Load models in stages with memory clearing between loads\n",
    "        print(\"Loading Stable Diffusion...\")\n",
    "        self.model = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float32,\n",
    "            safety_checker=None\n",
    "        ).to('cpu')    \n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "        print(\"Loading face depth estimator...\")\n",
    "        self.depth_estimator = FaceDepthEstimator('cpu')  # Initialize on CPU first\n",
    "        \n",
    "        # Then when moving to device, replace:\n",
    "        # self.depth_estimator.to(self.device)\n",
    "        # with:\n",
    "        self.depth_estimator = FaceDepthEstimator(self.device)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        self.rank = rank\n",
    "    \n",
    "        # Freeze parameters while still on CPU\n",
    "        self._freeze_parameters()\n",
    "    \n",
    "        # Add LoRA layers while on CPU\n",
    "        self._add_lora_layers()\n",
    "    \n",
    "        # Ensure all model parameters require gradients\n",
    "        for param in self.model.unet.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.model.vae.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.model.text_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "        print(\"Moving models to device...\")\n",
    "        # Move models to device in stages\n",
    "        try:\n",
    "            if torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "            self.depth_estimator = FaceDepthEstimator(device=self.device)\n",
    "            \n",
    "            if torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "            self.model.to(self.device)\n",
    "    \n",
    "        except RuntimeError as e:\n",
    "            print(\"Memory error during model loading. Trying to free memory...\")\n",
    "            if torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "            raise e\n",
    "\n",
    "    def _freeze_parameters(self):\n",
    "        \"\"\"Freeze all parameters in the base model\"\"\"\n",
    "        # Freeze UNet parameters\n",
    "        for param in self.model.unet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Freeze VAE parameters\n",
    "        for param in self.model.vae.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Freeze text encoder parameters\n",
    "        for param in self.model.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def _add_lora_layers(self):\n",
    "        self.lora_layers = []\n",
    "\n",
    "        def create_attention_forward(original_forward, lora):\n",
    "            def new_forward(self, x, *args, **kwargs):\n",
    "                x = lora(x)\n",
    "                return original_forward(self, x, *args, **kwargs)\n",
    "            return new_forward\n",
    "    \n",
    "        def create_custom_forward(original_forward, lora, heads):\n",
    "            def new_forward(self, hidden_states, encoder_hidden_states=None, attention_mask=None, **kwargs):\n",
    "                batch_size, sequence_length, _ = hidden_states.shape\n",
    "    \n",
    "                query = self.to_q(hidden_states)\n",
    "                query = lora(query)\n",
    "    \n",
    "                key = self.to_k(encoder_hidden_states if encoder_hidden_states is not None else hidden_states)\n",
    "                value = self.to_v(encoder_hidden_states if encoder_hidden_states is not None else hidden_states)\n",
    "    \n",
    "                # Use head size from query projection dimensions\n",
    "                head_size = query.shape[-1] // heads\n",
    "    \n",
    "                # Reshape attention patterns\n",
    "                query = query.reshape(batch_size, -1, heads, head_size).transpose(1, 2)\n",
    "                key = key.reshape(batch_size, -1, heads, head_size).transpose(1, 2)\n",
    "                value = value.reshape(batch_size, -1, heads, head_size).transpose(1, 2)\n",
    "    \n",
    "                # Compute attention scores\n",
    "                attention_scores = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(head_size)\n",
    "    \n",
    "                if attention_mask is not None:\n",
    "                    attention_scores = attention_scores + attention_mask\n",
    "    \n",
    "                attention_probs = attention_scores.softmax(dim=-1)\n",
    "                hidden_states = torch.matmul(attention_probs, value)\n",
    "    \n",
    "                # Reshape back\n",
    "                hidden_states = hidden_states.transpose(1, 2).contiguous()\n",
    "                hidden_states = hidden_states.reshape(batch_size, -1, heads * head_size)\n",
    "    \n",
    "                # Output projection\n",
    "                hidden_states = self.to_out[0](hidden_states)\n",
    "                hidden_states = self.to_out[1](hidden_states)\n",
    "    \n",
    "                return hidden_states\n",
    "            return new_forward\n",
    "    \n",
    "        for name, module in self.model.unet.named_modules():\n",
    "            if isinstance(module, torch.nn.MultiheadAttention):\n",
    "                in_dim = module.embed_dim\n",
    "                lora = LoRALayer(in_dim, self.rank).to(self.device)\n",
    "                self.lora_layers.append(lora)\n",
    "    \n",
    "                module.forward = create_attention_forward(\n",
    "                    module.forward,\n",
    "                    lora\n",
    "                ).__get__(module)\n",
    "    \n",
    "            elif \"attn\" in name and hasattr(module, 'to_q'):\n",
    "                in_dim = module.to_q.out_features\n",
    "                lora = LoRALayer(in_dim, self.rank).to(self.device)\n",
    "    \n",
    "                module.forward = create_custom_forward(\n",
    "                    module.forward,\n",
    "                    lora,\n",
    "                    module.heads\n",
    "                ).__get__(module)\n",
    "    \n",
    "                self.lora_layers.append(lora)\n",
    "\n",
    "    def estimate_depth(self, images):\n",
    "        \"\"\"Estimate depth using face-specific approach\"\"\"\n",
    "        if torch.is_tensor(images):\n",
    "            images = [transforms.ToPILImage()(img.cpu()) for img in images]\n",
    "    \n",
    "        depths = []\n",
    "        for image in images:\n",
    "            depth = self.depth_estimator(image)  # Using the class instance\n",
    "            depths.append(depth)\n",
    "    \n",
    "        depths = torch.stack(depths)\n",
    "        return depths\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        try:\n",
    "            if torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "\n",
    "            images = batch['image'].to(self.device)\n",
    "            print(f\"Input image shape: {images.shape}\")\n",
    "\n",
    "            # Get depth estimation as pseudo-ground truth\n",
    "            depths = self.estimate_depth(images)\n",
    "            depths = depths.squeeze()\n",
    "            if len(depths.shape) == 2:\n",
    "                depths = depths.unsqueeze(0).unsqueeze(0)\n",
    "            print(f\"Depths shape after processing: {depths.shape}\")\n",
    "\n",
    "            # Generate depth map directly without prompting\n",
    "            outputs = self.model(image=images, strength=1.0).images\n",
    "\n",
    "            # Process model outputs\n",
    "            outputs = torch.stack([transforms.ToTensor()(image) for image in outputs]).to(self.device)\n",
    "            outputs = outputs.mean(dim=1, keepdim=True)\n",
    "\n",
    "            # Resize depths if needed\n",
    "            depths = F.interpolate(depths, size=(outputs.shape[2], outputs.shape[3]), mode='bilinear', align_corners=False)\n",
    "            depths.requires_grad = True\n",
    "\n",
    "            loss = F.mse_loss(outputs, depths)\n",
    "            assert loss.requires_grad, \"Loss does not require gradients\"\n",
    "\n",
    "            return loss, outputs\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                if torch.backends.mps.is_available():\n",
    "                    torch.mps.empty_cache()\n",
    "                raise e\n",
    "            raise e\n",
    "        \n",
    "    def predict(self, image):\n",
    "        with torch.no_grad():\n",
    "            if torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "\n",
    "            output = self.model(\n",
    "                prompt=[\"depth map\"],\n",
    "                image=image.unsqueeze(0),\n",
    "                num_inference_steps=10,\n",
    "                guidance_scale=7.5\n",
    "            ).images[0]\n",
    "            return output\n",
    "\n",
    "    def verify_dimensions(self):\n",
    "        \"\"\"Verify that all dimensions match correctly\"\"\"\n",
    "        print(\"\\nVerifying dimensions:\")\n",
    "        for name, module in self.model.unet.named_modules():\n",
    "            if hasattr(module, 'to_q'):\n",
    "                print(f\"\\nModule: {name}\")\n",
    "                print(f\"Query projection: {module.to_q.in_features} -> {module.to_q.out_features}\")\n",
    "                print(f\"Number of heads: {module.heads}\")\n",
    "                print(f\"Head size: {module.to_q.out_features // module.heads}\")\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T20:49:29.396737Z",
     "start_time": "2024-11-25T20:49:28.405288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset implementation\n",
    "\n",
    "class LocalFaceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Get all image files\n",
    "        self.image_files = []\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
    "            self.image_files.extend(glob.glob(os.path.join(root_dir, ext)))\n",
    "        print(f\"Found {len(self.image_files)} images in {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return {'image': image}\n",
    "\n",
    "# Set up dataset with memory-efficient parameters\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),  # Reduced size for memory efficiency\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "dataset = LocalFaceDataset(\n",
    "    root_dir=\"faces\",  # Your local directory with face images\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create subset of 250 samples as per paper\n",
    "indices = random.sample(range(len(dataset)), min(100, len(dataset)))\n",
    "dataset = Subset(dataset, indices)\n",
    "\n",
    "# Use small batch size for memory efficiency\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "print(f\"Created dataloader with {len(dataset)} samples\")\n",
    "\n",
    "# Training function\n",
    "def train_ilora(model, dataloader, num_epochs=10):\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for layer in model.lora_layers for p in layer.parameters() if p.requires_grad],\n",
    "        lr=1e-4\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            try:\n",
    "                # Training step\n",
    "                loss, predictions = model.train_step(batch)\n",
    "\n",
    "                # Ensure loss requires gradients\n",
    "                if not loss.requires_grad:\n",
    "                    raise RuntimeError(\"Loss does not require gradients\")\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "                # Clear memory\n",
    "                if torch.backends.mps.is_available():\n",
    "                    torch.mps.empty_cache()\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(\"Memory error during training. Clearing cache and skipping batch...\")\n",
    "                    if torch.backends.mps.is_available():\n",
    "                        torch.mps.empty_cache()\n",
    "                    continue\n",
    "                raise e\n",
    "\n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Save checkpoint\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'unet_state_dict': model.model.unet.state_dict(),\n",
    "                'vae_state_dict': model.model.vae.state_dict(),\n",
    "                'text_encoder_state_dict': model.model.text_encoder.state_dict(),\n",
    "                'lora_layers_state_dict': [layer.state_dict() for layer in model.lora_layers],\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, f'ilora_checkpoint_epoch_{epoch+1}.pt')\n",
    "\n",
    "            # Visualize results for last batch\n",
    "            if len(dataloader) > 0:\n",
    "                sample_batch = next(iter(dataloader))\n",
    "                sample_image = sample_batch['image'][0]\n",
    "                estimated_depth = model.estimate_depth([sample_image])[0].squeeze().cpu().numpy()\n",
    "                pred_depth = model.predict(sample_image)\n",
    "\n",
    "                plt.figure(figsize=(15, 5))\n",
    "                plt.subplot(131)\n",
    "                plt.imshow(sample_image.permute(1, 2, 0))\n",
    "                plt.title('Original Image')\n",
    "                plt.subplot(132)\n",
    "                plt.imshow(estimated_depth, cmap='magma')\n",
    "                plt.title('Estimated Depth')\n",
    "                plt.subplot(133)\n",
    "                plt.imshow(pred_depth, cmap='magma')\n",
    "                plt.title(f'Predicted Depth (Epoch {epoch+1})')\n",
    "                plt.show()\n",
    "                plt.close()"
   ],
   "id": "61b5dc61054f16f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 202599 images in faces\n",
      "Created dataloader with 100 samples\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T20:52:19.270877Z",
     "start_time": "2024-11-25T20:49:29.406705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize and train\n",
    "try:\n",
    "    print(\"Initializing I-LoRA...\")\n",
    "    ilora = ILoRA()\n",
    "    ilora.verify_dimensions()\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_ilora(ilora, dataloader, num_epochs=10)\n",
    "\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e):\n",
    "        print(\"Memory error. Try reducing batch size or image size further.\")\n",
    "    raise e"
   ],
   "id": "51db236dfd14bc54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing I-LoRA...\n",
      "Using device: mps\n",
      "Loading Stable Diffusion...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "066523546c8342bf88002f3edfa507c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading face depth estimator...\n",
      "Moving models to device...\n",
      "\n",
      "Verifying dimensions:\n",
      "\n",
      "Module: down_blocks.0.attentions.0.transformer_blocks.0.attn1\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: down_blocks.0.attentions.0.transformer_blocks.0.attn2\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: down_blocks.0.attentions.1.transformer_blocks.0.attn1\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: down_blocks.0.attentions.1.transformer_blocks.0.attn2\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: down_blocks.1.attentions.0.transformer_blocks.0.attn1\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: down_blocks.1.attentions.0.transformer_blocks.0.attn2\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: down_blocks.1.attentions.1.transformer_blocks.0.attn1\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: down_blocks.1.attentions.1.transformer_blocks.0.attn2\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: down_blocks.2.attentions.0.transformer_blocks.0.attn1\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: down_blocks.2.attentions.0.transformer_blocks.0.attn2\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: down_blocks.2.attentions.1.transformer_blocks.0.attn1\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: down_blocks.2.attentions.1.transformer_blocks.0.attn2\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: up_blocks.1.attentions.0.transformer_blocks.0.attn1\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: up_blocks.1.attentions.0.transformer_blocks.0.attn2\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: up_blocks.1.attentions.1.transformer_blocks.0.attn1\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: up_blocks.1.attentions.1.transformer_blocks.0.attn2\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: up_blocks.1.attentions.2.transformer_blocks.0.attn1\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: up_blocks.1.attentions.2.transformer_blocks.0.attn2\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: up_blocks.2.attentions.0.transformer_blocks.0.attn1\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: up_blocks.2.attentions.0.transformer_blocks.0.attn2\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: up_blocks.2.attentions.1.transformer_blocks.0.attn1\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: up_blocks.2.attentions.1.transformer_blocks.0.attn2\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: up_blocks.2.attentions.2.transformer_blocks.0.attn1\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: up_blocks.2.attentions.2.transformer_blocks.0.attn2\n",
      "Query projection: 640 -> 640\n",
      "Number of heads: 8\n",
      "Head size: 80\n",
      "\n",
      "Module: up_blocks.3.attentions.0.transformer_blocks.0.attn1\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: up_blocks.3.attentions.0.transformer_blocks.0.attn2\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: up_blocks.3.attentions.1.transformer_blocks.0.attn1\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: up_blocks.3.attentions.1.transformer_blocks.0.attn2\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: up_blocks.3.attentions.2.transformer_blocks.0.attn1\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: up_blocks.3.attentions.2.transformer_blocks.0.attn2\n",
      "Query projection: 320 -> 320\n",
      "Number of heads: 8\n",
      "Head size: 40\n",
      "\n",
      "Module: mid_block.attentions.0.transformer_blocks.0.attn1\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Module: mid_block.attentions.0.transformer_blocks.0.attn2\n",
      "Query projection: 1280 -> 1280\n",
      "Number of heads: 8\n",
      "Head size: 160\n",
      "\n",
      "Starting training...\n",
      "Input image shape: torch.Size([1, 3, 64, 64])\n",
      "Depths shape after processing: torch.Size([1, 1, 384, 384])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m     ilora\u001B[38;5;241m.\u001B[39mverify_dimensions()\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mStarting training...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 8\u001B[0m     \u001B[43mtrain_ilora\u001B[49m\u001B[43m(\u001B[49m\u001B[43milora\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mout of memory\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e):\n",
      "Cell \u001B[0;32mIn[43], line 58\u001B[0m, in \u001B[0;36mtrain_ilora\u001B[0;34m(model, dataloader, num_epochs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     57\u001B[0m         \u001B[38;5;66;03m# Training step\u001B[39;00m\n\u001B[0;32m---> 58\u001B[0m         loss, predictions \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m         \u001B[38;5;66;03m# Ensure loss requires gradients\u001B[39;00m\n\u001B[1;32m     61\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mrequires_grad:\n",
      "Cell \u001B[0;32mIn[42], line 225\u001B[0m, in \u001B[0;36mILoRA.train_step\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDepths shape after processing: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdepths\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    224\u001B[0m \u001B[38;5;66;03m# Generate depth map directly without prompting\u001B[39;00m\n\u001B[0;32m--> 225\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrength\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mimages\n\u001B[1;32m    227\u001B[0m \u001B[38;5;66;03m# Process model outputs\u001B[39;00m\n\u001B[1;32m    228\u001B[0m outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack([transforms\u001B[38;5;241m.\u001B[39mToTensor()(image) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m outputs])\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/GPU/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/GPU/lib/python3.8/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py:979\u001B[0m, in \u001B[0;36mStableDiffusionImg2ImgPipeline.__call__\u001B[0;34m(self, prompt, image, strength, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001B[0m\n\u001B[1;32m    976\u001B[0m     callback_on_step_end_tensor_inputs \u001B[38;5;241m=\u001B[39m callback_on_step_end\u001B[38;5;241m.\u001B[39mtensor_inputs\n\u001B[1;32m    978\u001B[0m \u001B[38;5;66;03m# 1. Check inputs. Raise error if not correct\u001B[39;00m\n\u001B[0;32m--> 979\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_inputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    981\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstrength\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    982\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallback_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnegative_prompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    984\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprompt_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    985\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnegative_prompt_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    986\u001B[0m \u001B[43m    \u001B[49m\u001B[43mip_adapter_image\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    987\u001B[0m \u001B[43m    \u001B[49m\u001B[43mip_adapter_image_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    988\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallback_on_step_end_tensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    989\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    991\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_guidance_scale \u001B[38;5;241m=\u001B[39m guidance_scale\n\u001B[1;32m    992\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clip_skip \u001B[38;5;241m=\u001B[39m clip_skip\n",
      "File \u001B[0;32m/opt/anaconda3/envs/GPU/lib/python3.8/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py:676\u001B[0m, in \u001B[0;36mStableDiffusionImg2ImgPipeline.check_inputs\u001B[0;34m(self, prompt, strength, callback_steps, negative_prompt, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, callback_on_step_end_tensor_inputs)\u001B[0m\n\u001B[1;32m    671\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    672\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot forward both `prompt`: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprompt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and `prompt_embeds`: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprompt_embeds\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Please make sure to\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    673\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m only forward one of the two.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    674\u001B[0m     )\n\u001B[1;32m    675\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m prompt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m prompt_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 676\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    677\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProvide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    678\u001B[0m     )\n\u001B[1;32m    679\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m prompt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(prompt, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(prompt, \u001B[38;5;28mlist\u001B[39m)):\n\u001B[1;32m    680\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`prompt` has to be of type `str` or `list` but is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(prompt)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined."
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b077c2631196bc9c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
