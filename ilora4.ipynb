{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-25T21:39:57.282588Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoFeatureExtractor, AutoModelForDepthEstimation\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class LoRA(nn.Module):\n",
    "    def __init__(self, original_layer, rank):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        in_features, out_features = original_layer.weight.shape\n",
    "        self.rank = rank\n",
    "\n",
    "        # Ensure the rank is smaller than the input/output features\n",
    "        self.rank = min(self.rank, in_features, out_features)\n",
    "\n",
    "        self.lora_down = nn.Linear(in_features, self.rank, bias=False)\n",
    "        self.lora_up = nn.Linear(self.rank, out_features, bias=False)\n",
    "\n",
    "        self.scale = 0.1\n",
    "\n",
    "        nn.init.normal_(self.lora_down.weight, std=1.0 / self.rank)\n",
    "        nn.init.zeros_(self.lora_up.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        if len(x.shape) == 3:\n",
    "            seq_len = x.size(1)\n",
    "            x_reshape = x.view(batch_size * seq_len, -1)\n",
    "        else:\n",
    "            x_reshape = x\n",
    "\n",
    "        lora_output = self.lora_up(self.lora_down(x_reshape))\n",
    "        lora_output = lora_output * self.scale\n",
    "\n",
    "        if len(x.shape) == 3:\n",
    "            lora_output = lora_output.view(batch_size, seq_len, -1)\n",
    "\n",
    "        return self.original_layer(x) + lora_output\n",
    "\n",
    "class DepthEstimator:\n",
    "    def __init__(self, device='cpu'):\n",
    "        self.device = device\n",
    "        self.model = AutoModelForDepthEstimation.from_pretrained(\"intel/dpt-hybrid-midas\").to(device)\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(\"intel/dpt-hybrid-midas\")\n",
    "        self.model.eval()\n",
    "\n",
    "    def estimate_depth(self, images):\n",
    "        # Convert 4-channel RGBA images to 3-channel RGB\n",
    "        images_rgb = images[:, :3, :, :]\n",
    "\n",
    "        # Convert tensor to PIL images for feature extractor\n",
    "        images_np = ((images_rgb.clamp(-1, 1) + 1) * 127.5).byte().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "        images_list = [Image.fromarray(img) for img in images_np]\n",
    "\n",
    "        inputs = self.feature_extractor(images=images_list, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.predicted_depth\n",
    "\n",
    "class ILoraDepthTrainer:\n",
    "    def __init__(self, stable_diffusion_model, depth_estimator, rank=8, device='cpu'):\n",
    "        self.device = device\n",
    "        self.pipeline = stable_diffusion_model\n",
    "        self.depth_estimator = depth_estimator\n",
    "        self.replace_attention_layers(rank)\n",
    "\n",
    "    def replace_attention_layers(self, rank):\n",
    "        def _should_replace_layer(name, module):\n",
    "            attention_patterns = ['attn1', 'attn2']\n",
    "            return (isinstance(module, nn.Linear) and\n",
    "                    any(pattern in name for pattern in attention_patterns))\n",
    "\n",
    "        for name, module in self.pipeline.unet.named_modules():\n",
    "            if _should_replace_layer(name, module):\n",
    "                *parent_path, attr_name = name.split('.')\n",
    "                parent_module = self.pipeline.unet\n",
    "                for part in parent_path:\n",
    "                    parent_module = getattr(parent_module, part)\n",
    "                setattr(parent_module, attr_name, LoRA(module, rank).to(self.device))\n",
    "\n",
    "    def train(self, dataloader, optimizer, epochs=10):\n",
    "        self.pipeline.unet.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch_idx, (images,) in enumerate(dataloader):\n",
    "                # Normalize images to [-1, 1] range for the pipeline\n",
    "                images = images.to(self.device)\n",
    "\n",
    "                # Generate pseudo-ground truth depth\n",
    "                with torch.no_grad():\n",
    "                    pseudo_depth = self.depth_estimator.estimate_depth(images)\n",
    "\n",
    "                # Prepare inputs for the UNet\n",
    "                batch_size = images.size(0)\n",
    "                timesteps = torch.zeros(batch_size, device=self.device)\n",
    "                encoder_hidden_states = torch.randn(\n",
    "                    batch_size, 77, 768, device=self.device\n",
    "                )\n",
    "\n",
    "                # Forward pass\n",
    "                noise_pred = self.pipeline.unet(\n",
    "                    images,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=encoder_hidden_states\n",
    "                ).sample\n",
    "\n",
    "                # Normalize depths for comparison\n",
    "                pseudo_depth_norm = (pseudo_depth - pseudo_depth.min()) / (pseudo_depth.max() - pseudo_depth.min())\n",
    "                noise_pred_norm = (noise_pred - noise_pred.min()) / (noise_pred.max() - noise_pred.min())\n",
    "\n",
    "                loss = nn.MSELoss()(noise_pred_norm, pseudo_depth_norm)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "            avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch} completed. Average loss: {avg_epoch_loss:.6f}\")\n",
    "\n",
    "    def save_sample_outputs(self, images, epoch, batch_idx):\n",
    "        \"\"\"\n",
    "        Save sample outputs for visualization\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            pseudo_depth = self.depth_estimator.estimate_depth(images)\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            timesteps = torch.zeros(batch_size, device=self.device)\n",
    "            encoder_hidden_states = torch.randn(\n",
    "                batch_size, 77, 768, device=self.device\n",
    "            )\n",
    "\n",
    "            predicted_depth = self.pipeline.unet(\n",
    "                images,\n",
    "                timesteps,\n",
    "                encoder_hidden_states=encoder_hidden_states\n",
    "            ).sample\n",
    "\n",
    "        # Save the first image from the batch\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        # Original image\n",
    "        img_np = ((images[0].cpu().numpy() + 1) * 127.5).astype(np.uint8).transpose(1, 2, 0)\n",
    "        ax1.imshow(img_np)\n",
    "        ax1.set_title('Original Image')\n",
    "        ax1.axis('off')\n",
    "\n",
    "        # Ground truth depth\n",
    "        ax2.imshow(pseudo_depth[0].cpu().numpy(), cmap='viridis')\n",
    "        ax2.set_title('Ground Truth Depth')\n",
    "        ax2.axis('off')\n",
    "\n",
    "        # Predicted depth\n",
    "        ax3.imshow(predicted_depth[0].cpu().numpy(), cmap='viridis')\n",
    "        ax3.set_title('Predicted Depth')\n",
    "        ax3.axis('off')\n",
    "\n",
    "        plt.savefig(f'depth_comparison_epoch_{epoch}_batch_{batch_idx}.png')\n",
    "        plt.close()\n",
    "\n",
    "import torch\n",
    "\n",
    "def generate_normalized_images(batch_size, height, width):\n",
    "    \"\"\"\n",
    "    Generate random images normalized to [-1, 1] range with 4 channels (RGBA).\n",
    "    \"\"\"\n",
    "    images = torch.rand(batch_size, 4, height, width) * 2 - 1  # Range: [-1, 1]\n",
    "    return images\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = \"cpu\"\n",
    "\n",
    "    # Load models\n",
    "    stable_diffusion_model = StableDiffusionPipeline.from_pretrained(\n",
    "        \"CompVis/stable-diffusion-v1-4\",\n",
    "        safety_checker=None\n",
    "    ).to(device)\n",
    "    depth_estimator = DepthEstimator(device=device)\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = ILoraDepthTrainer(\n",
    "        stable_diffusion_model,\n",
    "        depth_estimator,\n",
    "        rank=4,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Create properly normalized example dataset\n",
    "    example_images = generate_normalized_images(8, 512, 512)  # 8 images, normalized to [-1, 1] with 4 channels\n",
    "    dataset = TensorDataset(example_images)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for p in trainer.pipeline.unet.parameters() if p.requires_grad],\n",
    "        lr=1e-4,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.train(dataloader, optimizer, epochs=5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c46a84d317194242a369ad2e56c0b159"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7da43e6bd3495d5a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
